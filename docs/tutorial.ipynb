{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tutorial\n",
    "Tutorial przygotowany z:\n",
    "- Python 3.7\n",
    "- Jupyter Labs\n",
    "- Virutal Environment\n",
    "\n",
    "# Spis treści:\n",
    "- [Keras Tutorial](#keras-tutorial)\n",
    "- [Instalacja z Repozytorium](#instalacja-z-repozytorium)\n",
    "- [Keras](#keras)\n",
    "- [Teoria](#podstawy-&-teoria)\n",
    "- [Użycie](#uzycie)\n",
    "\n",
    "Repozytorium z kodem: [Github](https://github.com/Isur/keras-training)\n",
    "# Instalacja z Repozytorium\n",
    "## Środowisko Wirtualne\n",
    "### Tworzenie\n",
    "Linux: `python3.7 -m venv venv`\n",
    "\n",
    "Windows: `py -m venv venv`\n",
    "### Aktywacja\n",
    "Linux: `source ./venv/bin/activate`\n",
    "\n",
    "Windows: `.\\venv\\Scripts\\activate`\n",
    "## Instalacja paczek\n",
    "`pip install -r req.txt`\n",
    "## Struktura projektu\n",
    "`venv` - folder środowiska wirtualnego\n",
    "\n",
    "`docs` - jupyter notebooks, dokumentacja\n",
    "\n",
    "`src` - pliki źródłowe\n",
    "\n",
    "`req.txt` - zależności\n",
    "\n",
    "`app.py` - plik wejściowy, do uruchomienia aplikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "## Czym jest Keras\n",
    "Keras jest wysokopoziomowym API dla sieci neuronowych, działa z wykorzystaniem `TensorFlow`, `CNTK` lub `Theano`.\n",
    "\n",
    "Dokumentacja [Keras.io](https://keras.io/)\n",
    "\n",
    "**Keras** znaczy \"róg\" w języku greckim.\n",
    "\n",
    "## Cechy\n",
    "- łatwy i szybki w implementacji - przyjazny dla użytkowników, modularny, rozszerzalny\n",
    "- wspiera konwolucyjne oraz rekurencyjne sieci jak i ich kombinacje\n",
    "- wykorzystuje CPU oraz GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podstawy & teoria\n",
    "## Modele\n",
    "Podstawą jest `model`, za pomocą którego możemy organizować warstwy.\n",
    "\n",
    "Podstawowym modelem jest `Sequential` - linowy stos wartsw.\n",
    "\n",
    "Możemy stworzyć model przez przekazanie listy warstw do konstruktora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodawanie kolejnych warstw odbywa się przez metodę `.add()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(32, input_dim=784))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kształt wejścia\n",
    "Dla modelu musi zostać określony kształt (`input shape`). Wystarczy to określić dla pierwszej warstwy. Jest kilka możliwości aby to zrobić:\n",
    "- przekazanie `input_shape` jako argument dla pierwszej warstwy,\n",
    "- przekazanie `input_dim` jako argument dla niektórych warstw 2D takich jak `Dense`, niektóre warstwy 3D przyjmują również `input_length`,\n",
    "- gdy trzeba określić rozmiar zestawu danych (fixed batch size) można przekazać argument `batch_size`. W przypadku podania `batch_size=32` oraz `input_shape(6,8)` do warstwy, to każdy zestaw danych będzie wymagał kształtu `(32, 6, 8)`\n",
    "\n",
    "Przykład warstw, które będą identyczne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Użycie\n",
    "## Wymagane importy\n",
    "W tym przykładzie wykorzystany będzie do wczytania danych `numpy`, oraz modele (`models`) i warstwy (`layers`) z `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie danych\n",
    "Jako przykładowe dane został wykorzystany dataset: [Prima Indians Diabetes](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv) ([Szczegóły](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names))\n",
    "Do wczytania danych z pliku wykorzystujemy bibliotekę `numpy` i jej metodę `loadtxt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "x = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane wczytane do `x` to nasze zmienne wejściowe, a `y` to zmienna wyjściowa. `y = f(x)`. Opis kolejnych kolumn z pliku csv:\n",
    "Wejściowe (`x`):\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "Wyjściowe (`y`):\n",
    "   1. Class variable (0 or 1)\n",
    "   \n",
    "Naszym celem będzie nauczyć sieć aby na podstawie podanych danych, dopasować klasę (`0` lub `1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie modelu\n",
    "Model za pomocą którego będziemy organizować warstwy to `Sequential`.\n",
    "Należy pamiętać o tym, aby wejściowa warstwa posiadała argument `input_dim` zgodny z liczbą danych wejściowych, czyli w naszym przypadku 8 ponieważ mamy 8 zmiennych.\n",
    "Dla przykładu wykorzystamy strukturę z w pełni połączonymi trzema warstwami (rozmiar sieci jest zależny od problemu jaki ma ona rozwiązywać).\n",
    "Połączone warstwy zapewnia nam klasa `Dense`, w której możemy sprecyzować liczbę neuronów i funkcję aktywacji.\n",
    "Jako funkcję aktywacji dla pierwszych dwóch warstw wykorzystamy `ReLu` (Rectified Linear Unit), a dla ostatniej funkcję sigmoidalną, która zapewni nam wynik między 0 a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak zdefiniowany model daje nam 3 warstwy, gdzie wejściem jest 8 zmiennych,\n",
    "- pierwsza warstwa obsługuje 16 neuronów oraz wykorzystuje funkcje aktywacji `relu`, tutaj należy pamiętać o tym, aby pierwsza warstwa miałą zdefiniowane wejście,\n",
    "- druga warstwa obsługuje 8 neuronów oraz wykorzystuje funkcje aktywacji `relu`,\n",
    "- trzecia, ostatnia warstwa z jednym neuronem z funkcją sigmoidalną."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kompilacja\n",
    "Po zdefiniowaniu modelu możemy go skompilować. Wykorzystany do tego jest `backend` w postaci biblioteki `Tensorflow`, `Theano` lub `CNTK`. Pozwala on na odpowiednie wykorzystanie naszego sprzętu (CPU, GPU).\n",
    "Podczas kompilacji należy sprecyzować pewne ustawienia dla naszej sieci, a są to:\n",
    "- funckja staty - `loss` - wykorzystawana do oszacowania zestawu wag, w tym przypadku będzie to funkcja krzyżowej entropii (cross entropy), gdzie w bibliotece Keras jest zdefiniowana jako `binary_crossentropy`,\n",
    "- optymalizator - `optimizer` - wybór algorytmu optymalizacyjnego, w tym przypadku będzie to `adam`,\n",
    "- metryki - `metrics` - metryki, które zbieramy jako wynik, w tym przypadku będzie to `accuracy`, czyli trafność dopasowań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dopasowanie\n",
    "Zdefiniowany i skompilowany model jest teraz gotowy do obliczeń. Możemy nauczyć go wykorzystując metodę `fit`.\n",
    "Musimy ustalić:\n",
    "- `epochs` - liczba przejść przez wszystkie wiersze trenowanego modelu,\n",
    "- `batch` - liczba próbek rozważanych przy każdym przejściu przed aktualizacja wag,\n",
    "- wejście - kolumny wejściowe,\n",
    "- wyjście - wyniki\n",
    "Liczby `epoch` i `batch` możemy dobierać metodą prób i błędów, aż uda nam się wystarczająco dobrze wytrenować model.\n",
    "Dla tego przykładu użyjemy następujących ustawień:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f60cc4d0c90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=250, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ustawienie `verbose=0` powoduje, że konsola nie wypisuje każdego przejścia.\n",
    "## Oszacowanie\n",
    "Wytrenowany model można teraz wykorzystać do oszacowania danych. Zbiór danych można podzielić na dane treningowe i dane do oceny, albo do obu przypadków wykorzystać ten sam zestaw danych.\n",
    "Szacowanie da nam stary modelu oraz trafność."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7864583134651184\n",
      "Data loss: 0.46383048842350644\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x,y, verbose=0)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Data loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idealna sytuacja byłaby wtedy, gdy trafność naszego modelu wynosiła bo 100%, a stata 0%, jednak jest to praktycznie niemożliwe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prognozy\n",
    "Wykorzystując metodę `predict`, gdzie dzięki wykorzystaniu funkcji sigmoidalnej mamy wyniki między 0 a 1, możemy je zaokrąglić i wykorzystać, lub `predict_classes` możemy wyznaczyć prognozy dla danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 1 (expected 1)\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0] => 0 (expected 0)\n",
      "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.248, 26.0] => 0 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x)\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "for i in range(7):\n",
    "    print('%s => %d (expected %d)' % (x[i].tolist(), rounded[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 1 (expected 1)\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0] => 0 (expected 0)\n",
      "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.248, 26.0] => 0 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(x)\n",
    "for i in range(7):\n",
    "    print('%s => %d (expected %d)' % (x[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zgodnie z wcześniej wyznaczoną trafnością, część danych może nie zostać przewidziana tak jak powinna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zapis modelu\n",
    "Model można zapisać do pliku w celu późniejszego użycia.\n",
    "Wykorzystując metode `save` możemy zapisać cały model wraz z:\n",
    "- wagami\n",
    "- architekturą\n",
    "- ustawieniami kompliacji\n",
    "- stanem optymalizatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./saved.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Można również zapisać dany model do pliku `json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"./model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytywanie modelu\n",
    "Wczytanie istniejącego modelu jest możliwe z wykorzystaniem metody `load_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "loaded_model = load_model(\"./saved.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy wyświetlić podsumowanie z informacjami na temat danego modelu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 289\n",
      "Trainable params: 289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy wykorzystać wczytany model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 78.65%\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 1 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 1 (expected 1)\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0] => 0 (expected 0)\n",
      "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.248, 26.0] => 0 (expected 1)\n",
      "[10.0, 115.0, 0.0, 0.0, 0.0, 35.3, 0.134, 29.0] => 1 (expected 0)\n",
      "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.158, 53.0] => 1 (expected 1)\n",
      "[8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.232, 54.0] => 0 (expected 1)\n",
      "[4.0, 110.0, 92.0, 0.0, 0.0, 37.6, 0.191, 30.0] => 0 (expected 0)\n",
      "[10.0, 168.0, 74.0, 0.0, 0.0, 38.0, 0.537, 34.0] => 1 (expected 1)\n",
      "[10.0, 139.0, 80.0, 0.0, 0.0, 27.1, 1.441, 57.0] => 1 (expected 0)\n",
      "[1.0, 189.0, 60.0, 23.0, 846.0, 30.1, 0.398, 59.0] => 1 (expected 1)\n",
      "[5.0, 166.0, 72.0, 19.0, 175.0, 25.8, 0.587, 51.0] => 1 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "new_dataset = loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "i = dataset[:,0:8]\n",
    "j = dataset[:,8]\n",
    "score = loaded_model.evaluate(i, j, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "predicts = loaded_model.predict_classes(i)\n",
    "for k in range(15):\n",
    "    print('%s => %d (expected %d)' % (i[k].tolist(), predicts[k], j[k]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37764bitvenvvenv533511c5ddd545769582bc317d89649e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
